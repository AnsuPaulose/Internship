{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product_name):\n",
    "    base_url = \"https://www.amazon.in\"\n",
    "    search_url = f\"{base_url}/s?k={product_name.replace(' ', '+')}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        products = soup.find_all(\"div\", class_=\"s-result-item\")\n",
    "\n",
    "        if products:\n",
    "            for product in products:\n",
    "                product_name = product.find(\"span\", class_=\"a-text-normal\").get_text(strip=True)\n",
    "                product_price = product.find(\"span\", class_=\"a-price\")\n",
    "                price = product_price.find(\"span\", class_=\"a-offscreen\").get_text(strip=True) if product_price else \"Price not available\"\n",
    "                product_link = product.find(\"a\", class_=\"a-link-normal\", href=True)['href']\n",
    "                print(f\"Product: {product_name}\\nPrice: {price}\\nLink: {base_url}{product_link}\\n\")\n",
    "        else:\n",
    "            print(\"No products found.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Amazon.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search on Amazon.in: \")\n",
    "    search_amazon(user_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca048b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8dbd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_product_details(product):\n",
    "    base_url = 'https://www.amazon.in'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    products_data = []\n",
    "\n",
    "    page_num = 1\n",
    "    while page_num <= 3:\n",
    "        search_url = f\"{base_url}/s?k={product.replace(' ', '+')}&page={page_num}\"\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "            if not products:\n",
    "                break \n",
    "\n",
    "            for product in products:\n",
    "                details = {}\n",
    "\n",
    "                product_name_tag = product.find('span', {'class': 'a-text-normal'})\n",
    "                if product_name_tag:\n",
    "                    details['Name of the Product'] = product_name_tag.text.strip()\n",
    "                else:\n",
    "                    details['Name of the Product'] = '-'\n",
    "\n",
    "                brand_tag = product.find('span', {'class': 'a-size-base-plus'})\n",
    "                if brand_tag:\n",
    "                    details['Brand Name'] = brand_tag.text.strip()\n",
    "                else:\n",
    "                    details['Brand Name'] = '-'\n",
    "\n",
    "                price_tag = product.find('span', {'class': 'a-offscreen'})\n",
    "                if price_tag:\n",
    "                    details['Price'] = price_tag.text.strip()\n",
    "                else:\n",
    "                    details['Price'] = '-'\n",
    "\n",
    "                return_tag = product.find('span', {'class': 'a-text-bold', 'dir': 'auto'})\n",
    "                if return_tag:\n",
    "                    details['Return/Exchange'] = return_tag.text.strip()\n",
    "                else:\n",
    "                    details['Return/Exchange'] = '-'\n",
    "\n",
    "                delivery_tag = product.find('span', {'class': 'a-text-bold', 'dir': 'auto'})\n",
    "                if delivery_tag:\n",
    "                    details['Expected Delivery'] = delivery_tag.text.strip()\n",
    "                else:\n",
    "                    details['Expected Delivery'] = '-'\n",
    "\n",
    "                availability_tag = product.find('span', {'class': 'a-size-base', 'aria-label': 'In Stock.'})\n",
    "                if availability_tag:\n",
    "                    details['Availability'] = 'In Stock'\n",
    "                else:\n",
    "                    details['Availability'] = 'Out of Stock'\n",
    "\n",
    "                product_url = product.find('a', {'class': 'a-link-normal'})\n",
    "                if product_url:\n",
    "                    details['Product URL'] = base_url + product_url['href']\n",
    "                else:\n",
    "                    details['Product URL'] = '-'\n",
    "\n",
    "                products_data.append(details)\n",
    "\n",
    "            page_num += 1\n",
    "        else:\n",
    "            print(\"Failed to fetch data from Amazon\")\n",
    "            break\n",
    "\n",
    "    return products_data\n",
    "\n",
    "def create_csv(data, filename='amazon_products.csv'):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data has been saved to '{filename}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the product you want to search for on Amazon: \")\n",
    "    products_data = extract_product_details(search_query)\n",
    "\n",
    "    if products_data:\n",
    "        create_csv(products_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382a33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012b1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def scrape_images(keyword, num_images=10):\n",
    "    url = f\"https://www.google.com/search?q={keyword}&tbm=isch\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "                      '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        image_elements = soup.find_all('img', class_='t0fcAb')\n",
    "\n",
    "        image_urls = []\n",
    "        for img in image_elements[:num_images]:\n",
    "            if 'src' in img.attrs:\n",
    "                image_urls.append(img['src'])\n",
    "\n",
    "        os.makedirs(keyword, exist_ok=True)\n",
    "\n",
    "        for i, img_url in enumerate(image_urls):\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(f\"{keyword}/image_{i+1}.jpg\", \"wb\") as f:\n",
    "                f.write(img_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'cars', 'Machine+Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "    for keyword in keywords:\n",
    "        scrape_images(keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9c274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(product):\n",
    "    base_url = f\"https://www.flipkart.com/search?q={product.replace(' ', '+')}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "                      '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "        products_data = []\n",
    "\n",
    "        for product in products:\n",
    "            details = {}\n",
    "\n",
    "            product_name = product.find('div', {'class': '_4rR01T'})\n",
    "            if product_name:\n",
    "                details['Smartphone name'] = product_name.text.strip()\n",
    "            else:\n",
    "                details['Smartphone name'] = '-'\n",
    "\n",
    "            brand_name = product.find('div', {'class': '_2WkVRV'})\n",
    "            if brand_name:\n",
    "                details['Brand Name'] = brand_name.text.split()[0].strip()\n",
    "            else:\n",
    "                details['Brand Name'] = '-'\n",
    "\n",
    "            price = product.find('div', {'class': '_30jeq3 _1_WHN1'})\n",
    "            if price:\n",
    "                details['Price'] = price.text.strip()\n",
    "            else:\n",
    "                details['Price'] = '-'\n",
    "\n",
    "            product_url = product.find('a', {'class': '_1fQZEK'})\n",
    "            if product_url:\n",
    "                details['Product URL'] = 'https://www.flipkart.com' + product_url['href']\n",
    "            else:\n",
    "                details['Product URL'] = '-'\n",
    "\n",
    "            # Other details might not be available on the search result page\n",
    "            details['Colour'] = '-'\n",
    "            details['RAM'] = '-'\n",
    "            details['Storage(ROM)'] = '-'\n",
    "            details['Primary Camera'] = '-'\n",
    "            details['Secondary Camera'] = '-'\n",
    "            details['Display Size'] = '-'\n",
    "            details['Battery Capacity'] = '-'\n",
    "\n",
    "            products_data.append(details)\n",
    "\n",
    "        return products_data\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Flipkart\")\n",
    "        return []\n",
    "\n",
    "def create_csv(data, filename='flipkart_smartphones.csv'):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data has been saved to '{filename}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "    smartphone_data = scrape_flipkart_smartphones(search_query)\n",
    "\n",
    "    if smartphone_data:\n",
    "        create_csv(smartphone_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec1522c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_coordinates(city_name):\n",
    "    base_url = f\"https://www.google.com/maps/search/{city_name.replace(' ', '+')}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "                      '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        coords_div = soup.find('meta', {'itemprop': 'geo'})\n",
    "        \n",
    "        if coords_div:\n",
    "            latitude = coords_div.get('content').split(',')[0]\n",
    "            longitude = coords_div.get('content').split(',')[1]\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Google Maps\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the name of the city: \")\n",
    "    lat, long = scrape_coordinates(city)\n",
    "\n",
    "    if lat and long:\n",
    "        print(f\"Latitude: {lat}, Longitude: {long}\")\n",
    "    else:\n",
    "        print(\"Coordinates not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78434d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294dc139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "\n",
    "        laptops_data = []\n",
    "\n",
    "        for laptop in laptops:\n",
    "            details = {}\n",
    "\n",
    "            details['Name'] = laptop.find('div', class_='TopNumbeHeading sticky-footer').text.strip()\n",
    "\n",
    "            specs = laptop.find_next('div', class_='specs').text.strip()\n",
    "            specs_list = specs.split('|')\n",
    "            for spec in specs_list:\n",
    "                key, value = spec.split(':')\n",
    "                details[key.strip()] = value.strip()\n",
    "\n",
    "            laptops_data.append(details)\n",
    "\n",
    "        return laptops_data\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Digit.in\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gaming_laptops_data = scrape_gaming_laptops()\n",
    "\n",
    "    if gaming_laptops_data:\n",
    "        for laptop in gaming_laptops_data:\n",
    "            print(laptop)\n",
    "    else:\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5ab86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = 'https://www.forbes.com/billionaires/'\n",
    "\n",
    "    # Fetch the webpage content\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        billionaire_rows = soup.find_all('div', class_='personName')\n",
    "\n",
    "        billionaires_data = []\n",
    "\n",
    "        for row in billionaire_rows:\n",
    "            details = {}\n",
    "\n",
    "            details['Rank'] = row.find_previous(class_='rank').text.strip()\n",
    "            details['Name'] = row.text.strip()\n",
    "\n",
    "            parent = row.parent.parent\n",
    "\n",
    "            details['Net worth'] = parent.find(class_='netWorth').text.strip()\n",
    "            details['Age'] = parent.find(class_='age').text.strip()\n",
    "            details['Citizenship'] = parent.find(class_='countryOfCitizenship').text.strip()\n",
    "            details['Source'] = parent.find(class_='source-column').text.strip()\n",
    "            details['Industry'] = parent.find(class_='category').text.strip()\n",
    "\n",
    "            billionaires_data.append(details)\n",
    "\n",
    "        return billionaires_data\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Forbes.com\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    billionaires_data = scrape_forbes_billionaires()\n",
    "\n",
    "    if billionaires_data:\n",
    "        for billionaire in billionaires_data:\n",
    "            print(billionaire)\n",
    "    else:\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c3914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3eb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-api-python-client\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "API_KEY = 'YOUR_API_KEY'\n",
    "\n",
    "def get_video_comments(video_id, max_results=500):\n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "    # Retrieve comments for the video\n",
    "    response = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        order='time',  # Order by time\n",
    "        maxResults=max_results\n",
    "    ).execute()\n",
    "\n",
    "    comments_data = []\n",
    "\n",
    "    for item in response['items']:\n",
    "        comment = item['snippet']['topLevelComment']\n",
    "        comment_text = comment['snippet']['textDisplay']\n",
    "        comment_upvotes = comment['snippet']['likeCount']\n",
    "        comment_time = comment['snippet']['publishedAt']\n",
    "        \n",
    "        comment_details = {\n",
    "            'Comment': comment_text,\n",
    "            'Upvotes': comment_upvotes,\n",
    "            'Time': comment_time\n",
    "        }\n",
    "        \n",
    "        comments_data.append(comment_details)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_id = input(\"Enter YouTube video ID: \")\n",
    "    comments = get_video_comments(video_id)\n",
    "\n",
    "    if comments:\n",
    "        for index, comment in enumerate(comments, start=1):\n",
    "            print(f\"Comment {index}:\")\n",
    "            print(f\"Text: {comment['Comment']}\")\n",
    "            print(f\"Upvotes: {comment['Upvotes']}\")\n",
    "            print(f\"Time: {comment['Time']}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No comments found or invalid video ID.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b03c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.hostelworld.com/find-a-hostel/london\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "hostels = soup.find_all('div', {'class': 'hostel'})\n",
    "\n",
    "data = []\n",
    "\n",
    "for hostel in hostels:\n",
    "    name = hostel.find('h2', {'class': 'hostel-name'}).text.strip()\n",
    "\n",
    "    distance = hostel.find('p', {'class': 'distance'}).text.strip()\n",
    "\n",
    "    ratings = hostel.find('div', {'class': 'ratings'}).text.strip()\n",
    "    reviews = hostel.find('div', {'class': 'reviews'}).text.strip()\n",
    "\n",
    "    private_price = hostel.find('span', {'class': 'private-price'}).text.strip()\n",
    "    dorm_price = hostel.find('span', {'class': 'dorm-price'}).text.strip()\n",
    "\n",
    "    facilities = hostel.find('div', {'class': 'facilities'}).text.strip()\n",
    "    description = hostel.find('div', {'class': 'description'}).text.strip()\n",
    "\n",
    "    data.append({\n",
    "        'name': name,\n",
    "        'distance': distance,\n",
    "        'ratings': ratings,\n",
    "        'reviews': reviews,\n",
    "        'private_price': private_price,\n",
    "        'dorm_price': dorm_price,\n",
    "        'facilities': facilities,\n",
    "        'description': description\n",
    "    })\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a7528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503fbf77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa9165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11644afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d18250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be77af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a550f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e295f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5434377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
