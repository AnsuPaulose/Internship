{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02852905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\ansu\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ansu\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ansu\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement requets (from versions: none)\n",
      "ERROR: No matching distribution found for requets\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20d3aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement requets (from versions: none)\n",
      "ERROR: No matching distribution found for requets\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc33c4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c030ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Header\n",
      "0  Wikipedia\\n\\nThe Free Encyclopedia\n",
      "1            1 000 000+\\n\\n\\narticles\n",
      "2              100 000+\\n\\n\\narticles\n",
      "3               10 000+\\n\\n\\narticles\n",
      "4                1 000+\\n\\n\\narticles\n",
      "5                  100+\\n\\n\\narticles\n",
      "6          Wikipedia is not for sale.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://www.wikipedia.org/\")\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "header_texts = []\n",
    "\n",
    "for header in headers:\n",
    "    header_texts.append(header.text.strip())\n",
    "\n",
    "df = pd.DataFrame({\"Header\": header_texts})\n",
    "\n",
    "print(df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036014b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07915de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f919b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a8f679",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m terms \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Iterate over the rows of the table\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Extract the name and term of office from the table cells\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     cells \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m     name \u001b[38;5;241m=\u001b[39m cells[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the list of former presidents\n",
    "table = soup.find('table')\n",
    "\n",
    "# Create empty lists to store the data\n",
    "names = []\n",
    "terms = []\n",
    "\n",
    "# Iterate over the rows of the table\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    # Extract the name and term of office from the table cells\n",
    "    cells = row.find_all('td')\n",
    "    name = cells[0].text.strip()\n",
    "    term = cells[1].text.strip()\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    names.append(name)\n",
    "    terms.append(term)\n",
    "\n",
    "# Create a dataframe using the collected data\n",
    "data = {'Name': names, 'Term of Office': terms}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cb11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d390c5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "               Team Matches Points Rating\n",
      "0        India\\nIND      55  6,640    121\n",
      "1    Australia\\nAUS      42  4,926    117\n",
      "2  South Africa\\nSA      34  3,750    110\n",
      "3     Pakistan\\nPAK      36  3,922    109\n",
      "4   New Zealand\\nNZ      43  4,399    102\n",
      "5      England\\nENG      38  3,777     99\n",
      "6     Sri Lanka\\nSL      47  4,134     88\n",
      "7   Bangladesh\\nBAN      44  3,836     87\n",
      "8  Afghanistan\\nAFG      30  2,533     84\n",
      "9   West Indies\\nWI      38  2,582     68\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "                  Player Team Rating\n",
      "0           Shubman Gill  IND    826\n",
      "1             Babar Azam  PAK    824\n",
      "2            Virat Kohli  IND    791\n",
      "3           Rohit Sharma  IND    769\n",
      "4        Quinton de Kock   SA    760\n",
      "5         Daryl Mitchell   NZ    750\n",
      "6           David Warner  AUS    745\n",
      "7  Rassie van der Dussen   SA    735\n",
      "8           Harry Tector  IRE    729\n",
      "9            Dawid Malan  ENG    729\n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "           Player Team Rating\n",
      "0  Keshav Maharaj   SA    741\n",
      "1  Josh Hazlewood  AUS    703\n",
      "2  Mohammed Siraj  IND    699\n",
      "3  Jasprit Bumrah  IND    685\n",
      "4      Adam Zampa  AUS    675\n",
      "5     Rashid Khan  AFG    667\n",
      "6   Kuldeep Yadav  IND    667\n",
      "7     Trent Boult   NZ    663\n",
      "8  Shaheen Afridi  PAK    650\n",
      "9  Mohammad Shami  IND    648\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data for ODI teams, batsmen, and bowlers\n",
    "def scrape_odi_rankings():\n",
    "    base_url = \"https://www.icc-cricket.com\"\n",
    "    \n",
    "    # Scrape top 10 ODI teams\n",
    "    teams_url = base_url + \"/rankings/mens/team-rankings/odi\"\n",
    "    teams_response = requests.get(teams_url)\n",
    "    teams_soup = BeautifulSoup(teams_response.content, \"html.parser\")\n",
    "    \n",
    "    teams_data = []\n",
    "    teams_table = teams_soup.find(\"table\", class_=\"table\")\n",
    "    teams_rows = teams_table.find_all(\"tr\")\n",
    "\n",
    "    for row in teams_rows[1:11]:  # Skipping the header row and getting top 10 teams\n",
    "        cols = row.find_all(\"td\")\n",
    "        team_name = cols[1].text.strip()\n",
    "        matches = cols[2].text.strip()\n",
    "        points = cols[3].text.strip()\n",
    "        rating = cols[4].text.strip()\n",
    "        teams_data.append({\"Team\": team_name, \"Matches\": matches, \"Points\": points, \"Rating\": rating})\n",
    "\n",
    "    # Scrape top 10 ODI batsmen\n",
    "    batsmen_url = base_url + \"/rankings/mens/player-rankings/odi/batting\"\n",
    "    batsmen_response = requests.get(batsmen_url)\n",
    "    batsmen_soup = BeautifulSoup(batsmen_response.content, \"html.parser\")\n",
    "\n",
    "    batsmen_data = []\n",
    "    batsmen_table = batsmen_soup.find(\"table\", class_=\"table\")\n",
    "    batsmen_rows = batsmen_table.find_all(\"tr\")\n",
    "\n",
    "    for row in batsmen_rows[1:11]:  # Skipping the header row and getting top 10 batsmen\n",
    "        cols = row.find_all(\"td\")\n",
    "        player_name = cols[1].text.strip()\n",
    "        team = cols[2].text.strip()\n",
    "        rating = cols[3].text.strip()\n",
    "        batsmen_data.append({\"Player\": player_name, \"Team\": team, \"Rating\": rating})\n",
    "\n",
    "    # Scrape top 10 ODI bowlers\n",
    "    bowlers_url = base_url + \"/rankings/mens/player-rankings/odi/bowling\"\n",
    "    bowlers_response = requests.get(bowlers_url)\n",
    "    bowlers_soup = BeautifulSoup(bowlers_response.content, \"html.parser\")\n",
    "\n",
    "    bowlers_data = []\n",
    "    bowlers_table = bowlers_soup.find(\"table\", class_=\"table\")\n",
    "    bowlers_rows = bowlers_table.find_all(\"tr\")\n",
    "\n",
    "    for row in bowlers_rows[1:11]:  # Skipping the header row and getting top 10 bowlers\n",
    "        cols = row.find_all(\"td\")\n",
    "        player_name = cols[1].text.strip()\n",
    "        team = cols[2].text.strip()\n",
    "        rating = cols[3].text.strip()\n",
    "        bowlers_data.append({\"Player\": player_name, \"Team\": team, \"Rating\": rating})\n",
    "\n",
    "    # Create dataframes\n",
    "    teams_df = pd.DataFrame(teams_data)\n",
    "    batsmen_df = pd.DataFrame(batsmen_data)\n",
    "    bowlers_df = pd.DataFrame(bowlers_data)\n",
    "\n",
    "    return teams_df, batsmen_df, bowlers_df\n",
    "\n",
    "# Run the function to scrape data\n",
    "top_10_teams, top_10_batsmen, top_10_bowlers = scrape_odi_rankings()\n",
    "\n",
    "# Display the dataframes\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(top_10_teams)\n",
    "\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(top_10_batsmen)\n",
    "\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(top_10_bowlers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab644105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b976a09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in women's cricket:\n",
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,429    163\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      20  1,768     88\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      14  1,074     77\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      24  1,602     67\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL for Women's ODI Team Rankings\n",
    "team_url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(team_url)\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the team rankings\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "# Iterate over the rows of the table and extract the required data\n",
    "for row in table.find_all('tr')[1:11]:  # Extract the top 10 teams\n",
    "    cols = row.find_all('td')\n",
    "    teams.append(cols[1].text.strip())\n",
    "    matches.append(cols[2].text.strip())\n",
    "    points.append(cols[3].text.strip())\n",
    "    ratings.append(cols[4].text.strip())\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "team_df = pd.DataFrame({\n",
    "    'Team': teams,\n",
    "    'Matches': matches,\n",
    "    'Points': points,\n",
    "    'Rating': ratings\n",
    "})\n",
    "\n",
    "print(\"Top 10 ODI teams in women's cricket:\")\n",
    "print(team_df)\n",
    "\n",
    "# Similarly, you can scrape ODI batting rankings and all-rounder rankings using the same approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e565d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe2e0a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Time, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the news articles on the page\n",
    "articles = soup.find_all(\"div\", class_=\"Card-title\")\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "\n",
    "# Extract the headline, time, and link for each news article\n",
    "for article in articles:\n",
    "    headline = article.find(\"a\").text.strip()\n",
    "    time = article.find(\"time\").text.strip()\n",
    "    link = article.find(\"a\")[\"href\"]\n",
    "    \n",
    "    headlines.append(headline)\n",
    "    times.append(time)\n",
    "    links.append(link)\n",
    "\n",
    "# Create a dictionary with the scraped data\n",
    "data = {\n",
    "    \"Headline\": headlines,\n",
    "    \"Time\": times,\n",
    "    \"News Link\": links\n",
    "}\n",
    "\n",
    "# Create a dataframe from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6529c65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed88871e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "article_list = []\n",
    "\n",
    "articles = soup.find_all('li', class_='article-list-item')\n",
    "\n",
    "for article in articles:\n",
    "    title = article.find('a', class_='js-article-title').text\n",
    "    authors = article.find('ul', class_='authors').text\n",
    "    published_date = article.find('span', class_='published-online').text\n",
    "    paper_url = article.find('a', class_='js-article-title')['href']\n",
    "    \n",
    "    article_list.append({\n",
    "        'Paper Title': title,\n",
    "        'Authors': authors,\n",
    "        'Published Date': published_date,\n",
    "        'Paper URL': paper_url\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(article_list)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ae291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
